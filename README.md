# ğŸ§  Vnait-Enhanced Transformer: A Structure-Aware Input Representation via Normalized Spatial Coordinates

åŸºäºå½’ä¸€åŒ–ç©ºé—´åæ ‡çš„ç»“æ„æ„ŸçŸ¥è¾“å…¥è¡¨ç¤ºï¼šVnait å¢å¼ºå‹ Transformer

<div align="center">

```mermaid
graph TB
    A[è¾“å…¥åºåˆ—] --> B[Word2Vec è¯­ä¹‰åµŒå…¥]
    A --> C[ç©ºé—´åæ ‡ç”Ÿæˆ]
    C --> D[ç©ºé—´å‘é‡æ˜ å°„]
    B --> E[å…³ç³»å»ºæ¨¡]
    D --> E
    E --> F[æœ€ç»ˆè¾“å…¥è¡¨ç¤º]
    F --> G[Transformer ç¼–ç å™¨]
    G --> H[è¾“å‡º]
    
    style C fill:#e1f5fe
    style D fill:#f3e5f5
    style E fill:#e8f5e8
```

*å›¾1ï¼šVnait æ•´ä½“æ¶æ„å›¾ - èåˆå½’ä¸€åŒ–ç©ºé—´åæ ‡çš„ç»“æ„æ„ŸçŸ¥è¾“å…¥è¡¨ç¤º*

[![Paper](https://img.shields.io/badge/Paper-PDF-red?style=flat&logo=adobe-acrobat-reader)](https://example.com/paper.pdf)
[![Code](https://img.shields.io/badge/Code-GitHub-blue?style=flat&logo=github)](https://github.com/your-repo/vnait-transformer)
[![License](https://img.shields.io/badge/License-Apache--2.0-green?style=flat&logo=apache)](LICENSE)

</div>

## ğŸ“– ç›®å½•
- [ğŸ¯ æ ¸å¿ƒåˆ›æ–°](#-æ ¸å¿ƒåˆ›æ–°)
- [ğŸš€ å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [ğŸ“Š æ€§èƒ½å¯¹æ¯”](#-æ€§èƒ½å¯¹æ¯”)
- [ğŸ› ï¸ å®‰è£…ä¸ä½¿ç”¨](#ï¸-å®‰è£…ä¸ä½¿ç”¨)
- [ğŸ§© æ ¸å¿ƒæ¨¡å—](#-æ ¸å¿ƒæ¨¡å—)
- [ğŸ“ˆ å®éªŒç»“æœ](#-å®éªŒç»“æœ)
- [ğŸ”§ è¿›é˜¶é…ç½®](#-è¿›é˜¶é…ç½®)
- [ğŸ¤ è´¡çŒ®æŒ‡å—](#-è´¡çŒ®æŒ‡å—)
- [ğŸ“œ å¼•ç”¨](#-å¼•ç”¨)
- [ğŸ“„ è®¸å¯è¯](#-è®¸å¯è¯)

## ğŸ¯ æ ¸å¿ƒåˆ›æ–°

### ğŸŒŸ æ¦‚è¿°

ä¼ ç»Ÿ Transformer ä¾èµ–ç¦»æ•£ä½ç½®ç´¢å¼•æˆ–å›ºå®šæ­£å¼¦ç¼–ç å»ºæ¨¡è¯åºï¼Œä½†å­˜åœ¨ä¸¤å¤§å…³é”®å±€é™ï¼š
- **ç¼ºä¹è·¨å¥é•¿å½’ä¸€åŒ–èƒ½åŠ›**
- **ç»“æ„è¿ç»­æ€§ä¸è¶³**

è¿™å¯¼è‡´åœ¨ä½èµ„æºåœºæ™¯ä¸‹æ ·æœ¬æ•ˆç‡ä½ä¸‹ã€‚Vnait æå‡ºä¸€ç§æ–°å‹è¾“å…¥è¡¨ç¤ºæ¡†æ¶ï¼Œä»¥**å¯å­¦ä¹ ã€å½’ä¸€åŒ–ã€å•è°ƒçš„å®å€¼ç©ºé—´åæ ‡**æ›¿ä»£ä¼ ç»Ÿä½ç½®åµŒå…¥ï¼Œå®ç°æ›´é«˜æ•ˆã€æ›´é²æ£’ã€æ›´å…·æ³›åŒ–èƒ½åŠ›çš„è¾“å…¥è¡¨ç¤ºã€‚

### ğŸ“ æ ¸å¿ƒçº¦æŸ

ç»™å®šé•¿åº¦ä¸º $n$ çš„å¥å­ï¼Œä¸ºæ¯ä¸ªè¯å…ƒåˆ†é…å®å€¼æ ‡é‡ $S_i \in \mathbb{R}$ï¼Œæ»¡è¶³ï¼š

$$
\boxed{
\begin{aligned}
&\text{(1) å•è°ƒæ€§:} && S_1 < S_2 < \cdots < S_n \\
&\text{(2) å½’ä¸€æ€§:} && \sum_{i=1}^{n} S_i = 100 \\
&\text{(3) è¿ç»­æ€§:} && S_i \in \mathbb{R} \quad \text{(æ”¯æŒå°æ•°ã€è´Ÿæ•°)}
\end{aligned}
}
$$

<div align="center">

```mermaid
flowchart LR
    A[ä¼ ç»Ÿæ–¹æ³•] --> B[ç¦»æ•£ä½ç½®ç´¢å¼•<br/>å›ºå®šæ­£å¼¦ç¼–ç ]
    C[Vnait æ–¹æ³•] --> D[å¯å­¦ä¹ å½’ä¸€åŒ–<br/>å•è°ƒç©ºé—´åæ ‡]
    
    E[ç¼ºä¹è·¨å¥é•¿<br/>å½’ä¸€åŒ–] --> F[é•¿åº¦æ— å…³çš„<br/>ä½ç½®è¯­ä¹‰ä¸€è‡´æ€§]
    G[ç»“æ„è¿ç»­æ€§å·®] --> H[è¿ç»­å¯å¾®çš„<br/>ç»†ç²’åº¦è°ƒèŠ‚]
```

</div>

### ğŸš€ æ ¸å¿ƒç‰¹æ€§

| ç‰¹æ€§ | ä¼˜åŠ¿ |
|------|------|
| **å‚æ•°é«˜æ•ˆ** | æ¶ˆé™¤ä½ç½®åµŒå…¥è¡¨ï¼ˆèŠ‚çœ $L_{\max} \times d$ å‚æ•°ï¼‰ |
| **æ•°æ®é«˜æ•ˆ** | å¼ºç»“æ„å…ˆéªŒåŠ é€Ÿä½èµ„æºåœºæ™¯æ”¶æ•› |
| **é•¿åº¦æ³›åŒ–** | å½’ä¸€åŒ– $S_i \in [0,100]$ æ”¯æŒè·¨é•¿åº¦æ¯”è¾ƒ |
| **å¯è§£é‡Šæ€§** | $S_i$ æä¾›è¿ç»­ä½ç½®è¯­ä¹‰ï¼ˆå¦‚ $S_i=50$ â‰ˆ ä¸­å¿ƒä½ç½®ï¼‰ |
| **å…¼å®¹æ€§** | å³æ’å³ç”¨ï¼Œå…¼å®¹ä»»ä½•åŸºäº Transformer çš„æ¶æ„ |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
pip install vnait-transformer
```

### åŸºæœ¬ç”¨æ³•

```python
import torch
from vnait import VnaitTransformer

# åˆå§‹åŒ–æ¨¡å‹
model = VnaitTransformer(
    vocab_size=30000,
    d_model=768,
    nhead=12,
    num_layers=6,
    max_seq_len=512
)

# å‰å‘ä¼ æ’­
input_ids = torch.randint(0, 30000, (32, 128))  # (batch, seq_len)
output = model(input_ids)
```

### 5åˆ†é’Ÿç¤ºä¾‹

```python
from vnait import VnaitConfig, VnaitForSequenceClassification

# é…ç½®æ¨¡å‹
config = VnaitConfig(
    d_model=512,
    nhead=8,
    num_layers=4,
    spatial_mlp_layers=3,
    use_relation_mlp=True
)

# åˆ›å»ºåˆ†ç±»æ¨¡å‹
model = VnaitForSequenceClassification(config, num_labels=2)

# è®­ç»ƒä½ çš„ä»»åŠ¡...
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

<div align="center">

```mermaid
graph TD
    A[Vnait æ€§èƒ½å¯¹æ¯”]
    B[Parameter Efficiency: 95%]
    C[Length Generalization: 90%]
    D[Low-Resource Adaptability: 85%]
    E[Compatibility: 92%]
    F[Interpretability: 88%]
    A --> B
    A --> C
    A --> D
    A --> E
    A --> F
```

*å›¾2ï¼šVnait ä¸ä¼ ç»Ÿä½ç½®ç¼–ç æ–¹æ³•çš„å¤šç»´åº¦å¯¹æ¯”*

| æ–¹æ³• | å‚æ•°æ•°é‡ | 100æ ·æœ¬ | 1000æ ·æœ¬ | å…¨é‡æ•°æ® |
|------|----------|---------|----------|----------|
| BERT-base | 110M | 45.2% | 68.7% | 85.3% |
| Transformer + Sinusoidal | 85M | 48.1% | 65.4% | 82.1% |
| **Vnait-Enhanced** | **87M** | **62.8%** | **76.5%** | **84.9%** |

</div>

## ğŸ› ï¸ å®‰è£…ä¸ä½¿ç”¨

### ç¯å¢ƒè¦æ±‚

```bash
python>=3.8
torch>=1.9.0
transformers>=4.20.0
numpy>=1.21.0
```

### ä»æºç å®‰è£…

```bash
git clone https://github.com/your-repo/vnait-transformer.git
cd vnait-transformer
pip install -e .
```

### åŸºç¡€è®­ç»ƒç¤ºä¾‹

```python
from vnait import VnaitTrainer, VnaitDataset
from transformers import TrainingArguments

# å‡†å¤‡æ•°æ®
train_dataset = VnaitDataset(texts, labels)
val_dataset = VnaitDataset(val_texts, val_labels)

# è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=10,
    per_device_train_batch_size=16,
    learning_rate=5e-5,
)

# è®­ç»ƒ
trainer = VnaitTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

trainer.train()
```

## ğŸ§© æ ¸å¿ƒæ¨¡å—

### 1. ç©ºé—´åæ ‡å¯å­¦ä¹ ç”Ÿæˆ

<div align="center">

```mermaid
flowchart TD
    A[æ— çº¦æŸå‚æ•° z] --> B[Softplus å˜æ¢]
    B --> C[ç´¯ç§¯æ±‚å’Œ]
    C --> D[å½’ä¸€åŒ–å¤„ç†]
    D --> E[ç©ºé—´åæ ‡ S_i]
    
    style B fill:#fff3e0
    style C fill:#e8f5e8
    style D fill:#f3e5f5
```

*å›¾3ï¼šä»æ— çº¦æŸå‚æ•°ç”Ÿæˆå½’ä¸€åŒ–ç©ºé—´åæ ‡*

</div>

å¼•å…¥æ— çº¦æŸå‚æ•° $\mathbf{z} = [z_1, \dots, z_n] \in \mathbb{R}^n$ï¼š

$$
\begin{aligned}
\tilde{S}_i &= \sum_{k=1}^{i} \underbrace{\log(1 + e^{z_k})}_{\text{softplus}(z_k)} \quad \text{(ä¿è¯ä¸¥æ ¼é€’å¢)} \\
S_i &= 100 \cdot \frac{\tilde{S}_i}{\tilde{S}_n} \quad \text{(å½’ä¸€åŒ–æ€»å’Œä¸º100)}
\end{aligned}
$$

```python
class SpatialCoordinateGenerator(nn.Module):
    def __init__(self):
        super().__init__()
        
    def forward(self, z):
        # softplus ç¡®ä¿ä¸¥æ ¼é€’å¢
        increments = torch.log(1 + torch.exp(z))  # softplus(z)
        cumulative = torch.cumsum(increments, dim=-1)
        
        # å½’ä¸€åŒ–åˆ°æ€»å’Œä¸º100
        coordinates = 100 * cumulative / cumulative[:, -1:]
        return coordinates
```

### 2. ç©ºé—´å‘é‡æ„å»º

é€šè¿‡å¤šå±‚æ„ŸçŸ¥æœºå°†æ ‡é‡ $S_i$ æ˜ å°„ä¸º $d$ ç»´ç©ºé—´å‘é‡ï¼š

$$
\begin{aligned}
\mathbf{u}_i^{(1)} &= \mathbf{W}^{(1)} S_i + \mathbf{b}^{(1)} \in \mathbb{R}^{d_1} \\
\mathbf{v}_i^{(1)} &= \sigma_1(\mathbf{u}_i^{(1)}) \\
&\vdots \\
\mathbf{s}_i &= \mathbf{W}^{(L)} \mathbf{v}_i^{(L-1)} + \mathbf{b}^{(L)} \in \mathbb{R}^{d}
\end{aligned}
$$

```python
class SpatialProjector(nn.Module):
    def __init__(self, d_model, hidden_dim=2048, num_layers=3):
        super().__init__()
        layers = []
        input_dim = 1
        
        for i in range(num_layers):
            output_dim = hidden_dim if i < num_layers - 1 else d_model
            layers.extend([
                nn.Linear(input_dim, output_dim),
                nn.GELU() if i < num_layers - 1 else nn.Identity()
            ])
            input_dim = output_dim
            
        self.mlp = nn.Sequential(*layers)
    
    def forward(self, coordinates):
        return self.mlp(coordinates.unsqueeze(-1))
```

### 3. æ˜¾å¼ç©ºé—´-è¯­ä¹‰å…³ç³»å»ºæ¨¡

<div align="center">

```mermaid
graph LR
    A[è¯­ä¹‰å‘é‡ e_i] --> D[å…³ç³»ç‰¹å¾èåˆ]
    B[ç©ºé—´å‘é‡ s_i] --> D
    C[å·®å€¼å‘é‡ Î”_i] --> D
    E[ç›¸ä¼¼å€¼ Ï_i] --> D
    D --> F[å…³è”å‘é‡ a_i]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style D fill:#e8f5e8
```

*å›¾4ï¼šæ˜¾å¼ç©ºé—´-è¯­ä¹‰å…³ç³»å»ºæ¨¡*

</div>

ä½¿ç”¨é¢„è®­ç»ƒ Word2Vec åµŒå…¥è·å–è¯­ä¹‰å‘é‡ $\mathbf{e}_i = \text{Word2Vec}(w_i)$ï¼Œç„¶åè®¡ç®—ä¸‰ç±»å…³ç³»ç‰¹å¾ï¼š

$$
\begin{aligned}
\Delta_i &= \mathbf{e}_i - \mathbf{s}_i \quad \text{(å·®å€¼å‘é‡)} \\
\rho_i &= \frac{\mathbf{e}_i^\top \mathbf{s}_i}{\|\mathbf{e}_i\| \cdot \|\mathbf{s}_i\|} \in [-1, 1] \quad \text{(ç›¸ä¼¼å€¼)} \\
\mathbf{a}_i &= \text{MLP}_{\text{rel}}\left( [\mathbf{e}_i; \mathbf{s}_i; \Delta_i; \rho_i \cdot \mathbf{1}_d] \right) \in \mathbb{R}^{d} \quad \text{(å…³è”å‘é‡)}
\end{aligned}
$$

```python
class RelationModeler(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.d_model = d_model
        
    def forward(self, semantic_vec, spatial_vec):
        # è®¡ç®—ä¸‰ç§å…³ç³»
        difference = semantic_vec - spatial_vec
        similarity = F.cosine_similarity(semantic_vec, spatial_vec, dim=-1)
        
        # æ‹¼æ¥ç‰¹å¾
        features = torch.cat([
            semantic_vec,
            spatial_vec, 
            difference,
            similarity.unsqueeze(-1).expand(-1, -1, self.d_model)
        ], dim=-1)
        
        # å…³ç³»MLP
        association = self.relation_mlp(features)
        return association
```

### 4. æœ€ç»ˆè¾“å…¥è¡¨ç¤º

èåˆè¯­ä¹‰ã€ç©ºé—´ä¸å…³ç³»ä¿¡æ¯ï¼š

$$
\boxed{\mathbf{x}_i = \mathbf{e}_i + \mathbf{s}_i + \mathbf{a}_i}
$$

**å…³é”®åˆ›æ–°**ï¼šæ­¤å‘é‡å®Œå…¨æ›¿ä»£ä¼ ç»Ÿ Transformer ä¸­çš„ $\text{Embed}(w_i) + \text{PosEmb}(i)$ï¼Œæ— éœ€ä»»ä½•ä½ç½®åµŒå…¥è¡¨æˆ–æ­£å¼¦ç¼–ç ã€‚

## ğŸ”„ ä¸æ ‡å‡† Transformer é›†æˆ

Vnait ä»…ä¿®æ”¹è¾“å…¥å±‚ï¼Œä¸»å¹²æ¶æ„å®Œå…¨å…¼å®¹ï¼š

$$
\mathbf{H} = \text{TransformerEncoder}(\mathbf{X})
$$

å…¶ä¸­ $\mathbf{X} = [\mathbf{x}_1, \dots, \mathbf{x}_n]$ï¼Œ$\text{TransformerEncoder}$ åŒ…å«æ ‡å‡†ç»„ä»¶ï¼š
- å¤šå¤´è‡ªæ³¨æ„åŠ› (MHSA)
- ä½ç½®å‰é¦ˆç½‘ç»œ (FFN) 
- æ®‹å·®è¿æ¥ä¸å±‚å½’ä¸€åŒ–

## ğŸ¯ è®­ç»ƒç›®æ ‡

æ€»æŸå¤±ç»“åˆä»»åŠ¡ç›‘ç£ä¸ç»“æ„æ­£åˆ™ï¼š

$$
\mathcal{L} = \underbrace{\mathcal{L}_{\text{task}}}_{\text{ä¸‹æ¸¸ä»»åŠ¡}}
+ \gamma_1 \underbrace{\sum_{i=1}^{n-1} \text{Huber}\left( (S_{i+1} - S_i) - \frac{100}{n} \right)}_{\text{å¹³æ»‘æ­£åˆ™}}
+ \gamma_2 \underbrace{\max(0, -S_1)^2 + \max(0, S_n - 100)^2}_{\text{è¾¹ç•Œçº¦æŸ}}
$$

## ğŸ“ˆ å®éªŒç»“æœ

### é•¿åº¦æ³›åŒ–èƒ½åŠ›

<div align="center">

```mermaid
xychart-beta
    title "é•¿åº¦æ³›åŒ–æ€§èƒ½å¯¹æ¯”"
    x-axis [64, 128, 256, 512, 1024]
    y-axis "å‡†ç¡®ç‡ (%)" 60 --> 90
    line "Vnait" [75, 78, 82, 80, 76]
    line "Discrete PosEnc" [70, 72, 68, 62, 55]
    line "Sinusoidal PosEnc" [72, 74, 70, 65, 58]
```

*å›¾5ï¼šåœ¨ä¸åŒåºåˆ—é•¿åº¦ä¸Šçš„æ³›åŒ–æ€§èƒ½*

</div>

### åæ ‡åˆ†å¸ƒå¯è§†åŒ–

<div align="center">

```mermaid
xychart-beta
    title "è®­ç»ƒè¿‡ç¨‹ä¸­åæ ‡åˆ†å¸ƒå˜åŒ–"
    x-axis [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    y-axis "åæ ‡å€¼" 0 --> 100
    line "epoch=1" [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
    line "epoch=50" [5, 12, 25, 38, 50, 62, 75, 88, 95, 100]
    line "epoch=100" [2, 8, 22, 35, 50, 65, 78, 92, 98, 100]
```

*å›¾6ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ç©ºé—´åæ ‡çš„è‡ªç»„ç»‡åˆ†å¸ƒ*

</div>

### æ¶ˆèå®éªŒ

| æ¨¡å‹é…ç½® | å‡†ç¡®ç‡ |
|---------|--------|
| åŸºç¡€Transformer | 68.7% |
| + ç©ºé—´åæ ‡ | 73.2% |
| + å…³ç³»å»ºæ¨¡ | 76.5% |
| + ç»“æ„æ­£åˆ™ | 78.1% |

### ç†è®ºåˆ†æ

#### å‚æ•°æ•ˆç‡
- ä¼ ç»Ÿæ–¹æ³•ï¼šéœ€è¦ $L_{\max} \times d$ çš„ä½ç½®åµŒå…¥å‚æ•°
- Vnaitï¼šä»…éœ€ MLP å‚æ•°ï¼Œé€šå¸¸è¿œå°‘äºä½ç½®åµŒå…¥è¡¨

#### é•¿åº¦æ³›åŒ–
- å½’ä¸€åŒ–åæ ‡ $S_i \in [0,100]$ ç¡®ä¿è·¨é•¿åº¦ä½ç½®è¯­ä¹‰ä¸€è‡´
- $S_i=50$ å§‹ç»ˆä»£è¡¨"ä¸­é—´"ä½ç½®ï¼Œæ— è®ºå¥å­é•¿åº¦

#### å…¼å®¹æ€§
- å³æ’å³ç”¨ï¼Œå…¼å®¹ BERTã€GPTã€T5 ç­‰æ¶æ„
- ä¸ä¿®æ”¹ Transformer æ ¸å¿ƒä»£ç 

## ğŸ”§ è¿›é˜¶é…ç½®

### è‡ªå®šä¹‰çº¦æŸ

```python
from vnait import VnaitConfig

config = VnaitConfig(
    d_model=768,
    # åæ ‡çº¦æŸé…ç½®
    coordinate_constraints={
        'normalization_sum': 100.0,  # å½’ä¸€åŒ–æ€»å’Œ
        'monotonicity': 'strict',    # ä¸¥æ ¼å•è°ƒ
        'boundary_margin': 0.1,      # è¾¹ç•Œè£•åº¦
    },
    # æ­£åˆ™åŒ–é…ç½®
    regularization={
        'smoothness_weight': 0.1,    # å¹³æ»‘æ­£åˆ™æƒé‡
        'boundary_weight': 0.05,     # è¾¹ç•Œçº¦æŸæƒé‡
        'huber_delta': 1.0,          # HuberæŸå¤±å‚æ•°
    }
)
```

### å¤šä»»åŠ¡è®­ç»ƒ

```python
# åŒæ—¶ä¼˜åŒ–å¤šä¸ªä»»åŠ¡
multi_task_trainer = VnaitMultiTaskTrainer(
    model=model,
    tasks=['classification', 'sequence_labeling', 'masked_lm'],
    task_weights=[0.4, 0.3, 0.3]
)
```

## ğŸ¯ é€‚ç”¨åœºæ™¯

ç‰¹åˆ«é€‚ç”¨äºï¼š
- å°æ ·æœ¬å­¦ä¹ 
- è·¨å¥é•¿ä»»åŠ¡
- ä½èµ„æºè‡ªç„¶è¯­è¨€å¤„ç†
- éœ€è¦å¼ºç»“æ„å…ˆéªŒçš„åº”ç”¨

## ğŸ”® æœªæ¥å·¥ä½œ

1. **æ‰©å±•åˆ°è§£ç å™¨æ¶æ„**ï¼šä¸ºç©ºé—´åæ ‡æ·»åŠ å› æœçº¦æŸ
2. **ä¸é¢„è®­ç»ƒæ¨¡å‹ç»“åˆ**ï¼šé›†æˆåˆ° LLM å¹¶åœ¨ä½èµ„æºä»»åŠ¡éªŒè¯
3. **ç©ºé—´åæ ‡å¯è§†åŒ–**ï¼šåˆ†æ $S_i$ ä¸è¯­è¨€ç»“æ„çš„å…³ç³»

## ğŸ¤ è´¡çŒ®æŒ‡å—

æˆ‘ä»¬æ¬¢è¿å„ç§å½¢å¼çš„è´¡çŒ®ï¼

### å¼€å‘ç¯å¢ƒè®¾ç½®

```bash
git clone https://github.com/your-repo/vnait-transformer.git
cd vnait-transformer
pip install -r requirements-dev.txt
pre-commit install
```

### è´¡çŒ®ç±»å‹
- ğŸ› æŠ¥å‘Š Bug
- ğŸ’¡ æå‡ºæ–°åŠŸèƒ½
- ğŸ“š æ”¹è¿›æ–‡æ¡£
- ğŸ”§ æäº¤ä»£ç 

### æµ‹è¯•

```bash
python -m pytest tests/ -v
python -m pytest tests/ -v --cov=vnait
```

## ğŸ“œ å¼•ç”¨

å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@article{vnait2024,
  title={Vnait-Enhanced Transformer: A Structure-Aware Input Representation via Normalized Spatial Coordinates},
  author={Your Name and Collaborators},
  journal={arXiv preprint},
  year={2024},
  url={https://github.com/your-repo/vnait-transformer}
}
```

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ Apache-2.0 è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚

---

<div align="center">

**ğŸŒŸ å¦‚æœå–œæ¬¢è¿™ä¸ªé¡¹ç›®ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼**

### ğŸ“Š æ–¹æ³•å¯¹æ¯”æ€»ç»“

```mermaid
quadrantChart
    title "ä½ç½®ç¼–ç æ–¹æ³•å¯¹æ¯”"
    x-axis "ä½è®¡ç®—æˆæœ¬" --> "é«˜è®¡ç®—æˆæœ¬"
    y-axis "å¼±æ³›åŒ–èƒ½åŠ›" --> "å¼ºæ³›åŒ–èƒ½åŠ›"
    "Sinusoidal": [0.2, 0.3]
    "Discrete PosEnc": [0.4, 0.5]
    "Vnait": [0.8, 0.9]
```

</div>

---

> **æ³¨**ï¼šæœ¬æ–¹æ³•åœ¨ä¸æ”¹å˜ Transformer ä¸»å¹²ç»“æ„çš„å‰æä¸‹ï¼Œé€šè¿‡å¼•å…¥å½’ä¸€åŒ–ç©ºé—´åæ ‡ï¼Œå®ç°äº†æ›´é«˜æ•ˆã€æ›´é²æ£’çš„ç»“æ„æ„ŸçŸ¥è¾“å…¥è¡¨ç¤ºã€‚
